{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMPjWt50XAdEBBltIO53a9T"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":12,"metadata":{"id":"dq_LxL3LzfMz","executionInfo":{"status":"ok","timestamp":1769174717849,"user_tz":-330,"elapsed":2,"user":{"displayName":"Prashant Raj","userId":"13131792136088247515"}}},"outputs":[],"source":["import torch\n","import torchvision\n","import matplotlib.pyplot as plt\n","import torchvision.transforms as transform\n","import torch.utils.data as dataloader\n","import torch.nn as nn"]},{"cell_type":"code","source":["batch_size = 64\n","num_classes = 10\n","img_size = 28\n","patch_size = 7\n","patch_num = (img_size // patch_size) ** 2\n","attn_heads = 4\n","embed_dim = 20\n","num_transformer_block = 4\n","mlp_nodes = 64\n","input_channel = 1\n","learning_rate = 0.001\n","epochs = 5\n","dropout = 0.1"],"metadata":{"id":"H9G23klv1h4-","executionInfo":{"status":"ok","timestamp":1769174717851,"user_tz":-330,"elapsed":1,"user":{"displayName":"Prashant Raj","userId":"13131792136088247515"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["transform = transform.Compose([\n","    transform.ToTensor(),\n","])"],"metadata":{"id":"8v7YOFuY0j2D","executionInfo":{"status":"ok","timestamp":1769174717862,"user_tz":-330,"elapsed":10,"user":{"displayName":"Prashant Raj","userId":"13131792136088247515"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["train_dataset = torchvision.datasets.MNIST(root='./data',train=True,download=True,transform=transform)\n","val_dataset = torchvision.datasets.MNIST(root='./data',train=False,download=True,transform=transform)"],"metadata":{"id":"h4YQcLYC0sFU","executionInfo":{"status":"ok","timestamp":1769174717977,"user_tz":-330,"elapsed":124,"user":{"displayName":"Prashant Raj","userId":"13131792136088247515"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["train_data = dataloader.DataLoader(dataset=train_dataset,batch_size=batch_size,shuffle=True)\n","val_data = dataloader.DataLoader(dataset=val_dataset,batch_size=batch_size,shuffle=True)"],"metadata":{"id":"JOT8lXp70913","executionInfo":{"status":"ok","timestamp":1769174717978,"user_tz":-330,"elapsed":14,"user":{"displayName":"Prashant Raj","userId":"13131792136088247515"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["class MultiheadAttention(nn.Module):\n","  def __init__(self,d_in,d_out,num_heads,dropout,context_len,qvk_bias = False):\n","    super().__init__()\n","    assert (d_out % num_heads == 0), \"Output Dimesion is not divisble by number of heads\"\n","\n","    self.d_out = d_out\n","    self.num_heads = num_heads\n","    self.head_dim = d_out // num_heads\n","\n","    self.W_query = nn.Linear(d_in,d_out,bias = qvk_bias)\n","    self.W_key = nn.Linear(d_in,d_out,bias = qvk_bias)\n","    self.W_value = nn.Linear(d_in,d_out,bias = qvk_bias)\n","\n","    self.out_proj = nn.Linear(d_out,d_out)\n","\n","    self.dropout = nn.Dropout(dropout)\n","\n","  def forward(self,x):\n","    b,num_tokens,d_in = x.shape #here d_in is embeding_dimesion\n","\n","    keys = self.W_key(x)\n","    query = self.W_query(x)\n","    value = self.W_value(x)\n","\n","    keys = keys.view(b,num_tokens,self.num_heads,self.head_dim)\n","    query = query.view(b,num_tokens,self.num_heads,self.head_dim)\n","    value = value.view(b,num_tokens,self.num_heads,self.head_dim)\n","\n","\n","    keys = keys.transpose(1,2)\n","    query = query.transpose(1,2)\n","    value = value.transpose(1,2)\n","\n","    attn_scores = query @ keys.transpose(-2,-1)\n","\n","    attn_weights = torch.softmax(attn_scores / (self.head_dim**0.5),dim = -1)\n","    attn_weights = self.dropout(attn_weights)\n","\n","    context_vec = (attn_weights @ value).transpose(1,2)\n","\n","    #context_vec = context_vec.flatten(2) it can also be used and works but it can create a copy so the space will increase\n","    context_vec = context_vec.contiguous().view(b,num_tokens,self.d_out)\n","\n","    context_vec = self.out_proj(context_vec)\n","\n","    return context_vec\n"],"metadata":{"id":"26xHFvpk7290","executionInfo":{"status":"ok","timestamp":1769174717978,"user_tz":-330,"elapsed":13,"user":{"displayName":"Prashant Raj","userId":"13131792136088247515"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["class PatchEmbedding(nn.Module):\n","  def __init__(self,input_channel,embed_dim,patch_size):\n","    super().__init__()\n","    self.patch_embed = nn.Conv2d(in_channels=input_channel,out_channels=embed_dim,kernel_size=patch_size,stride=patch_size)\n","\n","  def forward(self,x):\n","    x = self.patch_embed(x)\n","    x = x.flatten(2).transpose(1,2)\n","    return x\n","\n","\n","class LayerNorm(nn.Module):\n","  def __init__(self,embed_dim):\n","    super().__init__()\n","    self.eps = 1e-5\n","    self.scale = nn.Parameter(torch.ones(embed_dim))\n","    self.shift = nn.Parameter(torch.zeros(embed_dim))\n","  def forward(self,x):\n","    mean = x.mean(dim = -1,keepdim = True)\n","    var = x.var(dim = -1,keepdim = True,unbiased=False)\n","\n","    x = (x-mean) / torch.sqrt(var+self.eps)\n","\n","    return self.scale*x + self.shift\n","\n","class GELU(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    self.register_buffer('c', torch.sqrt(torch.tensor(2.0 / torch.pi)))\n","\n","  def forward(self, x):\n","    return 0.5 * x * (1 + torch.tanh(self.c * (x + 0.044715 * x.pow(3))))\n","\n","\n","class FeedForward(nn.Module):\n","  def __init__(self,embed_dim):\n","    super().__init__()\n","    self.layers = nn.Sequential(\n","        nn.Linear(in_features=embed_dim,out_features=4*embed_dim),\n","        GELU(),\n","        nn.Linear(in_features=4*embed_dim,out_features=embed_dim)\n","    )\n","  def forward(self,x):\n","    return self.layers(x)\n","\n","class MLP_head(nn.Module):\n","  def __init__(self,embed_dim):\n","    super().__init__()\n","    self.layer_norm1 = LayerNorm(embed_dim)\n","    self.mlp_head = nn.Linear(in_features=embed_dim,out_features=num_classes)\n","\n","  def forward(self,x):\n","    x = x[:, 0]   # CLS token\n","    x = self.layer_norm1(x)\n","    x = self.mlp_head(x)\n","\n","    return x"],"metadata":{"id":"t47zYcVS2QjC","executionInfo":{"status":"ok","timestamp":1769174717978,"user_tz":-330,"elapsed":8,"user":{"displayName":"Prashant Raj","userId":"13131792136088247515"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["class TransformerEncoder(nn.Module):\n","  def __init__(self,embed_dim,num_heads,dropout,max_seq_len) -> None:\n","    super().__init__()\n","    self.attn = MultiheadAttention(\n","        d_in=embed_dim,\n","        d_out=embed_dim,\n","        num_heads=num_heads,\n","        dropout=dropout,\n","        context_len=max_seq_len)\n","    self.ff = FeedForward(embed_dim)\n","    self.norm1 = LayerNorm(embed_dim)\n","    self.norm2 = LayerNorm(embed_dim)\n","    self.dropout_shortcut = nn.Dropout(dropout)\n","  def forward(self,x):\n","    shortcut = x\n","    x = self.norm1(x)\n","    x = self.attn(x)\n","    x = self.dropout_shortcut(x)\n","    x = shortcut + x\n","\n","    shortcut = x\n","    x = self.norm2(x)\n","    x = self.ff(x)\n","    x = self.dropout_shortcut(x)\n","    x = x + shortcut\n","\n","    return x"],"metadata":{"id":"NWpzUBS52I5N","executionInfo":{"status":"ok","timestamp":1769174718052,"user_tz":-330,"elapsed":76,"user":{"displayName":"Prashant Raj","userId":"13131792136088247515"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["class VisionTransformer(nn.Module):\n","  def __init__(self) -> None:\n","    super().__init__()\n","    self.patch_embedding = PatchEmbedding(input_channel,embed_dim,patch_size)\n","    self.cls_token = nn.Parameter(torch.randn(1,1,embed_dim))\n","    self.position_embedding = nn.Parameter(torch.randn((1,patch_num+1,embed_dim)))\n","    self.transformer_block = nn.Sequential(\n","       *[TransformerEncoder(embed_dim,attn_heads,dropout,patch_num+1) for _ in range(num_transformer_block)]\n","    )\n","    self.mlp_head = MLP_head(embed_dim)\n","\n","  def forward(self,x):\n","    x = self.patch_embedding(x)\n","    B = x.shape[0]\n","    cls_tokens = self.cls_token.expand(B,-1,-1)\n","    x = torch.concat((cls_tokens,x),dim = 1)\n","    x = x + self.position_embedding\n","    x = self.transformer_block(x)\n","    x = self.mlp_head(x)\n","\n","\n","    return x\n"],"metadata":{"id":"WTGjnuYJAKo8","executionInfo":{"status":"ok","timestamp":1769174718061,"user_tz":-330,"elapsed":2,"user":{"displayName":"Prashant Raj","userId":"13131792136088247515"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = VisionTransformer().to(device)\n","optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n","criterion = nn.CrossEntropyLoss()"],"metadata":{"id":"hofJFV1zAUrj","executionInfo":{"status":"ok","timestamp":1769174718063,"user_tz":-330,"elapsed":0,"user":{"displayName":"Prashant Raj","userId":"13131792136088247515"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["for epoch in range(epochs):\n","    model.train()\n","    running_loss = 0.0\n","    running_correct = 0\n","    running_total = 0\n","\n","    print(f\"\\nEpoch [{epoch+1}/{epochs}]\")\n","\n","    for batch_idx, (images, labels) in enumerate(train_data):\n","        images, labels = images.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","\n","        batch_size = labels.size(0)\n","        preds = outputs.argmax(dim=1)\n","        batch_correct = (preds == labels).sum().item()\n","        batch_acc = batch_correct / batch_size\n","\n","\n","        running_loss += loss.item() * batch_size\n","        running_correct += batch_correct\n","        running_total += batch_size\n","\n","        avg_loss = running_loss / running_total\n","        avg_acc = running_correct / running_total\n","\n","\n","        if (batch_idx + 1) % 50 == 0 or (batch_idx + 1) == len(train_data):\n","            print(\n","                f\"  Batch [{batch_idx+1}/{len(train_data)}] | \"\n","                f\"Batch Loss: {loss.item():.4f}, Batch Acc: {batch_acc:.4f} | \"\n","                f\"Avg Loss: {avg_loss:.4f}, Avg Acc: {avg_acc:.4f}\"\n","            )\n","\n","    model.eval()\n","    val_loss = 0.0\n","    val_correct = 0\n","    val_total = 0\n","\n","    with torch.no_grad():\n","        for images, labels in val_data:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","\n","            batch_size = labels.size(0)\n","            preds = outputs.argmax(dim=1)\n","\n","            val_loss += loss.item() * batch_size\n","            val_correct += (preds == labels).sum().item()\n","            val_total += batch_size\n","\n","    val_loss /= val_total\n","    val_acc = val_correct / val_total\n","\n","    print(\n","        f\"Epoch [{epoch+1}/{epochs}] Summary | \"\n","        f\"Train Loss: {avg_loss:.4f}, Train Acc: {avg_acc:.4f} | \"\n","        f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\"\n","    )\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HKpCXSf2CnUV","executionInfo":{"status":"ok","timestamp":1769174831522,"user_tz":-330,"elapsed":113458,"user":{"displayName":"Prashant Raj","userId":"13131792136088247515"}},"outputId":"93e8ddf1-fa93-4128-b775-e302e6d71b5d"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Epoch [1/5]\n","  Batch [50/938] | Batch Loss: 2.3012, Batch Acc: 0.1250 | Avg Loss: 2.3152, Avg Acc: 0.1178\n","  Batch [100/938] | Batch Loss: 1.8586, Batch Acc: 0.4062 | Avg Loss: 2.2232, Avg Acc: 0.1761\n","  Batch [150/938] | Batch Loss: 1.4177, Batch Acc: 0.4375 | Avg Loss: 2.0413, Avg Acc: 0.2551\n","  Batch [200/938] | Batch Loss: 1.1937, Batch Acc: 0.6250 | Avg Loss: 1.8863, Avg Acc: 0.3229\n","  Batch [250/938] | Batch Loss: 1.1543, Batch Acc: 0.5625 | Avg Loss: 1.7498, Avg Acc: 0.3773\n","  Batch [300/938] | Batch Loss: 1.0711, Batch Acc: 0.6094 | Avg Loss: 1.6370, Avg Acc: 0.4191\n","  Batch [350/938] | Batch Loss: 0.8970, Batch Acc: 0.7500 | Avg Loss: 1.5359, Avg Acc: 0.4593\n","  Batch [400/938] | Batch Loss: 0.7757, Batch Acc: 0.7500 | Avg Loss: 1.4393, Avg Acc: 0.4954\n","  Batch [450/938] | Batch Loss: 0.7723, Batch Acc: 0.7812 | Avg Loss: 1.3571, Avg Acc: 0.5259\n","  Batch [500/938] | Batch Loss: 0.6390, Batch Acc: 0.7656 | Avg Loss: 1.2888, Avg Acc: 0.5513\n","  Batch [550/938] | Batch Loss: 0.6323, Batch Acc: 0.7500 | Avg Loss: 1.2252, Avg Acc: 0.5749\n","  Batch [600/938] | Batch Loss: 0.6513, Batch Acc: 0.7969 | Avg Loss: 1.1718, Avg Acc: 0.5944\n","  Batch [650/938] | Batch Loss: 0.5459, Batch Acc: 0.7969 | Avg Loss: 1.1257, Avg Acc: 0.6116\n","  Batch [700/938] | Batch Loss: 0.3391, Batch Acc: 0.9062 | Avg Loss: 1.0819, Avg Acc: 0.6278\n","  Batch [750/938] | Batch Loss: 0.6054, Batch Acc: 0.8438 | Avg Loss: 1.0423, Avg Acc: 0.6423\n","  Batch [800/938] | Batch Loss: 0.4002, Batch Acc: 0.9062 | Avg Loss: 1.0058, Avg Acc: 0.6553\n","  Batch [850/938] | Batch Loss: 0.3803, Batch Acc: 0.8906 | Avg Loss: 0.9737, Avg Acc: 0.6676\n","  Batch [900/938] | Batch Loss: 0.8005, Batch Acc: 0.7500 | Avg Loss: 0.9447, Avg Acc: 0.6780\n","  Batch [938/938] | Batch Loss: 0.4248, Batch Acc: 0.8750 | Avg Loss: 0.9249, Avg Acc: 0.6852\n","Epoch [1/5] Summary | Train Loss: 0.9249, Train Acc: 0.6852 | Val Loss: 0.3186, Val Acc: 0.9072\n","\n","Epoch [2/5]\n","  Batch [50/938] | Batch Loss: 0.6747, Batch Acc: 0.8281 | Avg Loss: 0.4132, Avg Acc: 0.8734\n","  Batch [100/938] | Batch Loss: 0.4765, Batch Acc: 0.8438 | Avg Loss: 0.4048, Avg Acc: 0.8748\n","  Batch [150/938] | Batch Loss: 0.2794, Batch Acc: 0.9219 | Avg Loss: 0.3854, Avg Acc: 0.8811\n","  Batch [200/938] | Batch Loss: 0.6494, Batch Acc: 0.7812 | Avg Loss: 0.3769, Avg Acc: 0.8857\n","  Batch [250/938] | Batch Loss: 0.1685, Batch Acc: 0.9531 | Avg Loss: 0.3746, Avg Acc: 0.8851\n","  Batch [300/938] | Batch Loss: 0.3088, Batch Acc: 0.9219 | Avg Loss: 0.3718, Avg Acc: 0.8860\n","  Batch [350/938] | Batch Loss: 0.2222, Batch Acc: 0.9531 | Avg Loss: 0.3657, Avg Acc: 0.8882\n","  Batch [400/938] | Batch Loss: 0.2367, Batch Acc: 0.9375 | Avg Loss: 0.3648, Avg Acc: 0.8888\n","  Batch [450/938] | Batch Loss: 0.4300, Batch Acc: 0.8594 | Avg Loss: 0.3614, Avg Acc: 0.8891\n","  Batch [500/938] | Batch Loss: 0.4586, Batch Acc: 0.8594 | Avg Loss: 0.3593, Avg Acc: 0.8898\n","  Batch [550/938] | Batch Loss: 0.2898, Batch Acc: 0.8906 | Avg Loss: 0.3547, Avg Acc: 0.8909\n","  Batch [600/938] | Batch Loss: 0.2150, Batch Acc: 0.9375 | Avg Loss: 0.3489, Avg Acc: 0.8927\n","  Batch [650/938] | Batch Loss: 0.1738, Batch Acc: 0.9219 | Avg Loss: 0.3452, Avg Acc: 0.8936\n","  Batch [700/938] | Batch Loss: 0.2907, Batch Acc: 0.9062 | Avg Loss: 0.3403, Avg Acc: 0.8955\n","  Batch [750/938] | Batch Loss: 0.1795, Batch Acc: 0.9375 | Avg Loss: 0.3347, Avg Acc: 0.8973\n","  Batch [800/938] | Batch Loss: 0.3709, Batch Acc: 0.8750 | Avg Loss: 0.3321, Avg Acc: 0.8982\n","  Batch [850/938] | Batch Loss: 0.2985, Batch Acc: 0.8906 | Avg Loss: 0.3296, Avg Acc: 0.8989\n","  Batch [900/938] | Batch Loss: 0.2689, Batch Acc: 0.9062 | Avg Loss: 0.3268, Avg Acc: 0.9000\n","  Batch [938/938] | Batch Loss: 0.3611, Batch Acc: 0.8750 | Avg Loss: 0.3244, Avg Acc: 0.9007\n","Epoch [2/5] Summary | Train Loss: 0.3244, Train Acc: 0.9007 | Val Loss: 0.1994, Val Acc: 0.9390\n","\n","Epoch [3/5]\n","  Batch [50/938] | Batch Loss: 0.1251, Batch Acc: 0.9688 | Avg Loss: 0.2587, Avg Acc: 0.9203\n","  Batch [100/938] | Batch Loss: 0.3808, Batch Acc: 0.8906 | Avg Loss: 0.2588, Avg Acc: 0.9216\n","  Batch [150/938] | Batch Loss: 0.2558, Batch Acc: 0.9531 | Avg Loss: 0.2571, Avg Acc: 0.9214\n","  Batch [200/938] | Batch Loss: 0.3346, Batch Acc: 0.8750 | Avg Loss: 0.2494, Avg Acc: 0.9240\n","  Batch [250/938] | Batch Loss: 0.2705, Batch Acc: 0.9219 | Avg Loss: 0.2520, Avg Acc: 0.9243\n","  Batch [300/938] | Batch Loss: 0.3358, Batch Acc: 0.8906 | Avg Loss: 0.2515, Avg Acc: 0.9241\n","  Batch [350/938] | Batch Loss: 0.2953, Batch Acc: 0.8906 | Avg Loss: 0.2522, Avg Acc: 0.9233\n","  Batch [400/938] | Batch Loss: 0.1451, Batch Acc: 0.9531 | Avg Loss: 0.2508, Avg Acc: 0.9233\n","  Batch [450/938] | Batch Loss: 0.4588, Batch Acc: 0.8750 | Avg Loss: 0.2489, Avg Acc: 0.9235\n","  Batch [500/938] | Batch Loss: 0.1411, Batch Acc: 0.9531 | Avg Loss: 0.2494, Avg Acc: 0.9237\n","  Batch [550/938] | Batch Loss: 0.2555, Batch Acc: 0.9062 | Avg Loss: 0.2480, Avg Acc: 0.9239\n","  Batch [600/938] | Batch Loss: 0.3983, Batch Acc: 0.8750 | Avg Loss: 0.2481, Avg Acc: 0.9239\n","  Batch [650/938] | Batch Loss: 0.2639, Batch Acc: 0.9375 | Avg Loss: 0.2455, Avg Acc: 0.9249\n","  Batch [700/938] | Batch Loss: 0.3767, Batch Acc: 0.9219 | Avg Loss: 0.2445, Avg Acc: 0.9250\n","  Batch [750/938] | Batch Loss: 0.1664, Batch Acc: 0.9531 | Avg Loss: 0.2442, Avg Acc: 0.9250\n","  Batch [800/938] | Batch Loss: 0.1546, Batch Acc: 0.9531 | Avg Loss: 0.2422, Avg Acc: 0.9255\n","  Batch [850/938] | Batch Loss: 0.1862, Batch Acc: 0.8906 | Avg Loss: 0.2405, Avg Acc: 0.9258\n","  Batch [900/938] | Batch Loss: 0.0940, Batch Acc: 0.9688 | Avg Loss: 0.2393, Avg Acc: 0.9262\n","  Batch [938/938] | Batch Loss: 0.1912, Batch Acc: 0.9375 | Avg Loss: 0.2385, Avg Acc: 0.9266\n","Epoch [3/5] Summary | Train Loss: 0.2385, Train Acc: 0.9266 | Val Loss: 0.1507, Val Acc: 0.9534\n","\n","Epoch [4/5]\n","  Batch [50/938] | Batch Loss: 0.2825, Batch Acc: 0.9219 | Avg Loss: 0.1786, Avg Acc: 0.9416\n","  Batch [100/938] | Batch Loss: 0.3232, Batch Acc: 0.9219 | Avg Loss: 0.1865, Avg Acc: 0.9417\n","  Batch [150/938] | Batch Loss: 0.2232, Batch Acc: 0.9062 | Avg Loss: 0.1941, Avg Acc: 0.9406\n","  Batch [200/938] | Batch Loss: 0.2285, Batch Acc: 0.9531 | Avg Loss: 0.1991, Avg Acc: 0.9385\n","  Batch [250/938] | Batch Loss: 0.3668, Batch Acc: 0.9219 | Avg Loss: 0.1990, Avg Acc: 0.9383\n","  Batch [300/938] | Batch Loss: 0.1828, Batch Acc: 0.9375 | Avg Loss: 0.2024, Avg Acc: 0.9367\n","  Batch [350/938] | Batch Loss: 0.1210, Batch Acc: 0.9531 | Avg Loss: 0.2041, Avg Acc: 0.9366\n","  Batch [400/938] | Batch Loss: 0.2309, Batch Acc: 0.9375 | Avg Loss: 0.2013, Avg Acc: 0.9377\n","  Batch [450/938] | Batch Loss: 0.2129, Batch Acc: 0.9219 | Avg Loss: 0.2019, Avg Acc: 0.9373\n","  Batch [500/938] | Batch Loss: 0.0530, Batch Acc: 0.9844 | Avg Loss: 0.1976, Avg Acc: 0.9384\n","  Batch [550/938] | Batch Loss: 0.2223, Batch Acc: 0.9531 | Avg Loss: 0.1986, Avg Acc: 0.9378\n","  Batch [600/938] | Batch Loss: 0.0369, Batch Acc: 1.0000 | Avg Loss: 0.1971, Avg Acc: 0.9385\n","  Batch [650/938] | Batch Loss: 0.1141, Batch Acc: 0.9688 | Avg Loss: 0.1954, Avg Acc: 0.9393\n","  Batch [700/938] | Batch Loss: 0.1271, Batch Acc: 0.9688 | Avg Loss: 0.1956, Avg Acc: 0.9391\n","  Batch [750/938] | Batch Loss: 0.2048, Batch Acc: 0.9375 | Avg Loss: 0.1940, Avg Acc: 0.9396\n","  Batch [800/938] | Batch Loss: 0.3352, Batch Acc: 0.8906 | Avg Loss: 0.1947, Avg Acc: 0.9394\n","  Batch [850/938] | Batch Loss: 0.3114, Batch Acc: 0.9219 | Avg Loss: 0.1936, Avg Acc: 0.9401\n","  Batch [900/938] | Batch Loss: 0.3246, Batch Acc: 0.8906 | Avg Loss: 0.1936, Avg Acc: 0.9401\n","  Batch [938/938] | Batch Loss: 0.2401, Batch Acc: 0.9375 | Avg Loss: 0.1929, Avg Acc: 0.9402\n","Epoch [4/5] Summary | Train Loss: 0.1929, Train Acc: 0.9402 | Val Loss: 0.1300, Val Acc: 0.9597\n","\n","Epoch [5/5]\n","  Batch [50/938] | Batch Loss: 0.1461, Batch Acc: 0.9688 | Avg Loss: 0.1958, Avg Acc: 0.9391\n","  Batch [100/938] | Batch Loss: 0.1441, Batch Acc: 0.9375 | Avg Loss: 0.1937, Avg Acc: 0.9380\n","  Batch [150/938] | Batch Loss: 0.1730, Batch Acc: 0.9219 | Avg Loss: 0.1813, Avg Acc: 0.9435\n","  Batch [200/938] | Batch Loss: 0.1341, Batch Acc: 0.9688 | Avg Loss: 0.1762, Avg Acc: 0.9458\n","  Batch [250/938] | Batch Loss: 0.2925, Batch Acc: 0.9062 | Avg Loss: 0.1773, Avg Acc: 0.9450\n","  Batch [300/938] | Batch Loss: 0.1254, Batch Acc: 0.9375 | Avg Loss: 0.1769, Avg Acc: 0.9451\n","  Batch [350/938] | Batch Loss: 0.1267, Batch Acc: 0.9531 | Avg Loss: 0.1741, Avg Acc: 0.9460\n","  Batch [400/938] | Batch Loss: 0.1189, Batch Acc: 0.9531 | Avg Loss: 0.1740, Avg Acc: 0.9462\n","  Batch [450/938] | Batch Loss: 0.0612, Batch Acc: 0.9844 | Avg Loss: 0.1725, Avg Acc: 0.9470\n","  Batch [500/938] | Batch Loss: 0.1341, Batch Acc: 0.9531 | Avg Loss: 0.1705, Avg Acc: 0.9478\n","  Batch [550/938] | Batch Loss: 0.1208, Batch Acc: 0.9531 | Avg Loss: 0.1695, Avg Acc: 0.9479\n","  Batch [600/938] | Batch Loss: 0.2052, Batch Acc: 0.9531 | Avg Loss: 0.1701, Avg Acc: 0.9475\n","  Batch [650/938] | Batch Loss: 0.2326, Batch Acc: 0.9688 | Avg Loss: 0.1691, Avg Acc: 0.9475\n","  Batch [700/938] | Batch Loss: 0.1044, Batch Acc: 0.9688 | Avg Loss: 0.1683, Avg Acc: 0.9478\n","  Batch [750/938] | Batch Loss: 0.1614, Batch Acc: 0.9531 | Avg Loss: 0.1683, Avg Acc: 0.9478\n","  Batch [800/938] | Batch Loss: 0.3377, Batch Acc: 0.9062 | Avg Loss: 0.1680, Avg Acc: 0.9480\n","  Batch [850/938] | Batch Loss: 0.3103, Batch Acc: 0.9531 | Avg Loss: 0.1680, Avg Acc: 0.9481\n","  Batch [900/938] | Batch Loss: 0.2943, Batch Acc: 0.9219 | Avg Loss: 0.1679, Avg Acc: 0.9482\n","  Batch [938/938] | Batch Loss: 0.0576, Batch Acc: 1.0000 | Avg Loss: 0.1673, Avg Acc: 0.9483\n","Epoch [5/5] Summary | Train Loss: 0.1673, Train Acc: 0.9483 | Val Loss: 0.1037, Val Acc: 0.9666\n"]}]},{"cell_type":"code","source":["from torchsummary import summary"],"metadata":{"id":"nczGZtEoDs0O","executionInfo":{"status":"ok","timestamp":1769175254293,"user_tz":-330,"elapsed":6,"user":{"displayName":"Prashant Raj","userId":"13131792136088247515"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["print(model)"],"metadata":{"id":"yHwsJMVyp9L8","executionInfo":{"status":"ok","timestamp":1769175343634,"user_tz":-330,"elapsed":9,"user":{"displayName":"Prashant Raj","userId":"13131792136088247515"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"0510ff63-5f1c-423c-d38f-38aa9ae0c952"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["VisionTransformer(\n","  (patch_embedding): PatchEmbedding(\n","    (patch_embed): Conv2d(1, 20, kernel_size=(7, 7), stride=(7, 7))\n","  )\n","  (transformer_block): Sequential(\n","    (0): TransformerEncoder(\n","      (attn): MultiheadAttention(\n","        (W_query): Linear(in_features=20, out_features=20, bias=False)\n","        (W_key): Linear(in_features=20, out_features=20, bias=False)\n","        (W_value): Linear(in_features=20, out_features=20, bias=False)\n","        (out_proj): Linear(in_features=20, out_features=20, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=20, out_features=80, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=80, out_features=20, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (dropout_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","    (1): TransformerEncoder(\n","      (attn): MultiheadAttention(\n","        (W_query): Linear(in_features=20, out_features=20, bias=False)\n","        (W_key): Linear(in_features=20, out_features=20, bias=False)\n","        (W_value): Linear(in_features=20, out_features=20, bias=False)\n","        (out_proj): Linear(in_features=20, out_features=20, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=20, out_features=80, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=80, out_features=20, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (dropout_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","    (2): TransformerEncoder(\n","      (attn): MultiheadAttention(\n","        (W_query): Linear(in_features=20, out_features=20, bias=False)\n","        (W_key): Linear(in_features=20, out_features=20, bias=False)\n","        (W_value): Linear(in_features=20, out_features=20, bias=False)\n","        (out_proj): Linear(in_features=20, out_features=20, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=20, out_features=80, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=80, out_features=20, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (dropout_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","    (3): TransformerEncoder(\n","      (attn): MultiheadAttention(\n","        (W_query): Linear(in_features=20, out_features=20, bias=False)\n","        (W_key): Linear(in_features=20, out_features=20, bias=False)\n","        (W_value): Linear(in_features=20, out_features=20, bias=False)\n","        (out_proj): Linear(in_features=20, out_features=20, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=20, out_features=80, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=80, out_features=20, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (dropout_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","  )\n","  (mlp_head): MLP_head(\n","    (layer_norm1): LayerNorm()\n","    (mlp_head): Linear(in_features=20, out_features=10, bias=True)\n","  )\n",")\n"]}]},{"cell_type":"code","source":["print(\"Params:\", sum(p.numel() for p in model.parameters()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0ybzsuxQEWCg","executionInfo":{"status":"ok","timestamp":1769175346925,"user_tz":-330,"elapsed":5,"user":{"displayName":"Prashant Raj","userId":"13131792136088247515"}},"outputId":"fe6af9db-ebdf-4b20-d9fb-6386c1f48c19"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["Params: 21610\n"]}]}]}